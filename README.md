# screren-sudoku-finder — обнаружение судоку на экране (ветка)

**Краткое описание**

Ветка `screren-sudoku-finder` проекта *The-sudoku-9x9-solver-TS9S* посвящена модулю обнаружения положения поля судоку на экране с помощью модели YOLOv8. Цель — надежно локализовать область 9×9 сетки на скриншотах/видеопотоке, чтобы следующая стадия (распознавание цифр и решение) получала корректно вырезанный фрагмент.

---

## Содержание этого README

1. Назначение ветки
2. Краткая структура репозитория
3. Подробный план сбора датасета (как это делалось с помощью скриптов в корне)
4. Подготовка и разметка (формат YOLO)
5. Обучение (команда, гиперпараметры)
6. Структура данных и примеры команд
7. Замечания по приватности и постобработке
8. Как воспроизвести

---

## 1. Назначение ветки

Эта ветка реализует этап **object detection**: поиск и локализация поля судоку на изображении/скриншоте. Результат — веса для нейросети для определения координат bounding box, которые используют последующие модули распознавания цифр и решения головоломки.

## 2. Краткая структура репозитория (важные файлы)

- `datasetting.py` — основной набор скриптов/пайплайн для автоматизированного сбора, преобразования и подготовки изображений/аннотаций.
- `swapper.py` — вспомогательные утилиты для генерации вариаций изображений (замена фона/перестановки, комбинирование шаблонов) и приведения датасета к единообразию.
- `data.yml` — конфигурация датасета в формате, ожидаемом YOLO (train/val paths, классы).
- `images/` — директория с исходными изображениями (скриншоты, кадры).
- `labels/` — YOLO-метки в формате `class x_center y_center width height` (нормированные значения).
- `runs/detect/train/` — папка с результатами обучения (логи, веса, графики).


## 3. Подробный план сбора датасета (как это делалось с помощью скриптов в корне)

Ниже описан пошаговый план, который отражает реальный рабочий процесс подготовки датасета для обучения YOLOv8. В проекте этот процесс автоматизировали с помощью `datasetting.py` и `swapper.py`.

### Шаг A — Сбор исходных изображений
1. Собирались скриншоты экрана, содержащие поля судоку на фоне разных приложений: веб‑игры, PDF‑сканов, фотографии экранов телефонов/мониторов, рабочий стол.
2. Для приватности лица/личные данные на изображениях **размывались (blur)** перед загрузкой на гитхаб.

### Шаг B — Автоматическая фильтрация и предобработка (`datasetting.py`)
`datasetting.py` выполнял (или автоматизировал) следующие операции:
- чтение исходной папки `images/` и отбор изображений нужного разрешения/формата;
- применение blur для приватных регионов (опционально) — при пакетной обработке все входные изображения проходили фильтр размытия;
- приведение разрешения и соотношения сторон (скейлинг/центровка);
- детекция контуров/грид‑подсказки (опционально) для облегчения разметки;
- генерация предварительных bbox (если используется «weak supervision») и подготовка к ручной проверке/коррекции.

> Примечание: часть шагов могла быть интерактивной — скрипт создаёт csv/json со списком изображений и предполагаемых bbox, после чего разработчик проверяет и корректирует разметку.

### Шаг C — Разметка и генерация YOLO-лейблов
1. Разметка bounding box'ов производилась в формате YOLO (один файл `.txt` на изображение) с записью одной строки на объект: `class x_center y_center width height` (в относительных координатах).
2. Если применялись аугментации или синтетические комбинации (см. `swapper.py`), скрипты автоматически пересчитывали и записывали соответствующие метки.

### Шаг D — Аугментации и синтетика (`swapper.py`)
`swapper.py` использовался для перемешки изображений в папке учебы и валидации

После подготовки наборы файлов были разделены по соотношению 80/20 и пути были записаны в `data.yml`.


## 4. Подготовка и разметка (формат YOLO)
- Формат меток: по одной `.txt` на изображение с строками `class x_center y_center width height` в относительных координатах.
- Категория: одна — `sudoku` (class 0).
- `data.yml` указывает пути `train:` и `val:` и содержит `nc: 1` и `names: ['sudoku']`.


## 5. Обучение
Обучение модели проводилось командой (как в ветке):

```
yolo detect train data=/content/data.yml model=yolov8x.pt epochs=200 imgsz=1440 batch=4 device=cuda:0
```

Пояснения к параметрам:
- `model=yolov8x.pt` — предобученная большая версия YOLOv8 (transfer learning);
- `epochs=200` — количество эпох обучения;
- `imgsz=1440` — входной размер изображения (высокое разрешение для точной локализации сетки);
- `batch=4` — размер батча (возможно ограничен VRAM);
- `device=cuda:0` — обучение на GPU.

Результаты и артефакты обучения сохранялись в `runs/detect/train/` (веса, графики потерь, метрики).


## 6. Структура итогового датасета
```
dataset/
├─ images/
│  ├─ img-1-1.jpg
│  ├─ img-1-2.jpg
│  └─ ...
├─ labels/
│  ├─ img-1-1.txt
│  ├─ img-1-2.txt
│  └─ ...
└─ data.yml
```

`data.yml` пример:

```yaml
train: ./dataset/images/train
val: ./dataset/images/val
nc: 1
names: ['sudoku']
```


## 7. Замечания по приватности и этике
- Все изображения перед загрузкой/аннотированием были **заблюрены** для сокрытия личных данных — лица, e‑mailы, номера телефонов и т.п.

## 8. Как воспроизвести (быстрая инструкция)
1. Создать виртуальное окружение и установить зависимости (YOLOv8, OpenCV, Pillow и т.д.).
2. Подготовить папку `dataset/images/` с исходными снимками (или взять `images/` из репозитория, если блюр не помешает).

```bash
python datasetting.py 
# опционально
python swapper.py 
```

4. Проверить `data.yml`, затем запустить обучение:

```bash
yolo detect train data=/content/data.yml model=yolov8x.pt epochs=200 imgsz=1440 batch=4 device=cuda:0
```

5. Результаты в `runs/detect/train/`.

---

